{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tamil_Number_MNIST",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswit3/Tamil_Number_MNIST_Research/blob/master/Tamil_Number_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU-Rs4eccHMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxcNncnDGx1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!unzip drive/My\\ Drive/datasets/TAMIL_MNIST_DATASET/v1/TAMIL-MNIST-DATASET.zip -d drive/My\\ Drive/datasets/TAMIL_MNIST_DATASET/v1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgsRwYEdcgg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# import glob\n",
        "# import csv\n",
        "\n",
        "# from os import listdir\n",
        "# from os.path import isfile, join\n",
        "\n",
        "# Dir = \"drive/My Drive/datasets/TAMIL_MNIST_DATASET/v1/TAMIL-MNIST-DATASET/\"\n",
        "# folders = [f for f in glob.glob(Dir + \"**/\", recursive=True)]\n",
        "\n",
        "# for fn in folders[1:]:\n",
        "#   j = (fn[len(fn)-2:]).split(\"/\")[0]\n",
        "#   changeDir = Dir+j+\"/\"\n",
        "\n",
        "#   os.rename(fn, Dir+j)\n",
        "\n",
        "#   fileList = [f for f in listdir(changeDir) if isfile(join(changeDir, f))]\n",
        "#   for i, fname in enumerate(fileList):\n",
        "#     i += 1\n",
        "#     os.rename(changeDir+fname, changeDir+\"{}_{}.jpg\".format(j,i))\n",
        "#   print(\"done\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_s9K21jufbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import glob\n",
        "\n",
        "import cv2\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKn1oW7Mn_Ca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data = []\n",
        "IMG_SIZE = 100\n",
        "\n",
        "for i in range(1,10):\n",
        "\n",
        "  ImgDir = \"drive/My Drive/datasets/TAMIL_MNIST_DATASET/v1/TAMIL-MNIST-DATASET/{}/\".format(i)\n",
        "\n",
        "  fileList = [ImgDir+f for f in listdir(ImgDir) if isfile(join(ImgDir, f))]\n",
        "\n",
        "  for path1 in fileList:\n",
        "    if os.path.isfile(path1):\n",
        "        img1 = cv2.imread(path1, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array = cv2.resize(img1, (IMG_SIZE, IMG_SIZE))\n",
        "        training_data.append([new_array, i])\n",
        "        # plt.imshow(new_array, cmap='gray')\n",
        "        # plt.show()     \n",
        "    else:\n",
        "        print (\"The file \" + path1 + \" does not exist.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kls9VKP98QgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(training_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8vG5DyG9PHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "random.shuffle(training_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTUaB_-k_H47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for sample in training_data[:10]:\n",
        "  print(sample[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9_1C8KA_NnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for features,label in training_data:\n",
        "  X.append(features)\n",
        "  y.append(label)\n",
        "\n",
        "print(X[0].reshape(-1, IMG_SIZE, IMG_SIZE, 1))\n",
        "\n",
        "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoEQ4DV4_fJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSgcVNfR_zQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "Dir = \"drive/My Drive/datasets/TAMIL_MNIST_DATASET/v1/TAMIL-MNIST-DATASET/\"\n",
        "\n",
        "pickle_out = open(Dir+\"X.pickle\",\"wb\")\n",
        "pickle.dump(X, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "pickle_out = open(Dir+\"y.pickle\",\"wb\")\n",
        "pickle.dump(y, pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxlM_FsIAlUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1235hcZVAMdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle_in = open(Dir+\"X.pickle\",\"rb\")\n",
        "X = pickle.load(pickle_in)\n",
        "\n",
        "pickle_in = open(Dir+\"y.pickle\",\"rb\")\n",
        "y = pickle.load(pickle_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBxyb7eFAzx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = X/255.0\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(256, (4, 4), input_shape=X.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "\n",
        "model.add(Dense(64))\n",
        "\n",
        "model.add(Dense(9))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X, y, batch_size=1, epochs=10, validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}